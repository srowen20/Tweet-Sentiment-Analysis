{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "522d572e-767c-4734-a4da-98837f7306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Condition\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import random\n",
    "import yaml\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "acd8af13-a9a7-4dc6-ae46-7afe0adca4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PIPELINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "60371912-4224-424d-9bcb-f2e978319510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline runtime for versioning:\n",
    "\n",
    "pipeline_runtime = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# import config variables:\n",
    "with open('kubeflow_config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "locals().update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "6dea92b6-f850-438c-94cb-c58abb18f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def get_model_resource_name(project: str,\n",
    "                  location: str,\n",
    "                  model_display_name: str\n",
    "                  ) -> str:\n",
    "    \"\"\"\n",
    "    Extract AutoML model resource name to use as our model\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project,\n",
    "                    location=location)\n",
    "    models = aiplatform.Model.list(filter=f\"display_name={model_display_name}\", order_by='create_time')\n",
    "\n",
    "    return models[-1].resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "03ae1248-2d12-40f3-9880-4dadd17acb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def get_model_number(model_parent: str\n",
    "                  ) -> str:\n",
    "    \"\"\"\n",
    "    Extract AutoML model resource name to use as our model\n",
    "    \"\"\"\n",
    "\n",
    "    model_number = model_parent.split('/')[-1]\n",
    "\n",
    "    return model_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ffb11c1a-df58-4d5f-a808-99db0d85b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def preprocess_data(project: str,\n",
    "                    location: str,\n",
    "                    is_inference_data: bool,\n",
    "                    service_location: str,\n",
    "                    gcs_file: str,\n",
    "                    bq_raw_table: str,\n",
    "                    bq_processed_table: str,\n",
    "                    bq_dataset: str,\n",
    "                    preprocess_data_job_name: str,\n",
    "                    preprocess_data_image_uri: str,\n",
    "                    preprocess_data_machine_type: str,\n",
    "                    service_account: str,\n",
    "                    output_csv: str\n",
    "                   ) -> str:\n",
    "    \"\"\"\n",
    "    Runs out beam processing job\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import aiplatform as ai\n",
    "    from time import sleep\n",
    "    from datetime import datetime\n",
    "    \n",
    "    api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    client = ai.gapic.JobServiceClient(client_options=client_options)\n",
    "    \n",
    "    args = ['--project=' + project,\n",
    "            '--region=' + location,\n",
    "            '--runner=DataflowRunner',\n",
    "            '--temp_location=' + service_location,\n",
    "            '--requirements_file=job-requirements.txt',\n",
    "            '--is-inference-data=' + str(is_inference_data),\n",
    "            '--gcs-file=' + gcs_file,\n",
    "            '--raw-bq-table=' + bq_raw_table,\n",
    "            '--processed-bq-table=' + bq_processed_table,\n",
    "            '--bq-dataset=' + bq_dataset,\n",
    "            '--gcp-project=' + project,\n",
    "            '--gcs-output-csv=' + output_csv,\n",
    "           ]\n",
    "    \n",
    "    custom_job = {\n",
    "        \"display_name\": preprocess_data_job_name + '-' + datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "        \"job_spec\": {\n",
    "            \"service_account\": service_account,\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": preprocess_data_machine_type,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"disk_spec\": {\n",
    "                        \"boot_disk_type\": \"pd-ssd\",\n",
    "                        \"boot_disk_size_gb\": 1000\n",
    "                    },\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": preprocess_data_image_uri,\n",
    "                        \"command\": [],\n",
    "                        \"args\": args\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "    \n",
    "    try:\n",
    "        response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while submitting custom job. Error {e}\")\n",
    "        raise Exception(\"Unable to submit raw data load job error\")\n",
    "    else:\n",
    "        print(f\"Submitted custom job\")\n",
    "        \n",
    "    end_job = False\n",
    "    \n",
    "    # Wait for the job to reach end state\n",
    "    while not end_job:\n",
    "        get_response = client.get_custom_job(name=response.name)\n",
    "        \n",
    "        print(f\"Custom job state : {get_response.state.name}\")\n",
    "        \n",
    "        if get_response.state.name in ['JOB_STATE_SUCCEEDED',\n",
    "                                       'JOB_STATE_FAILED',\n",
    "                                       'JOB_STATE_CANCELLED',\n",
    "                                       'JobState.JOB_STATE_PAUSED',\n",
    "                                       'JobState.JOB_STATE_EXPIRED']:\n",
    "            end_job = True\n",
    "        else:\n",
    "            print(\"Waiting for the job to complete\")\n",
    "            sleep(120)\n",
    "    \n",
    "    if get_response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Custom job completed successfully\")\n",
    "    else:\n",
    "        print(f\"Custom job is not successful. Job state {get_response.state.name}\")\n",
    "        raise Exception(\"Custom job is not successful\")\n",
    "        \n",
    "    return bq_processed_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "db26b4b8-2fa0-49a7-bbc5-426c679d3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"pandas\",  \"db-dtypes\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def bq_table_to_csv(\n",
    "                    project: str,\n",
    "                    bq_dataset: str,\n",
    "                    bq_processed_table: str,\n",
    "                    bucket_name: str,\n",
    "                    processed_file_name: str\n",
    "                    ) -> str:\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas\n",
    "    \n",
    "    client1 = bigquery.Client()\n",
    "\n",
    "    dataset_ref = bigquery.DatasetReference(project, bq_dataset)\n",
    "    table_ref = dataset_ref.table(bq_processed_table)\n",
    "    table = client1.get_table(table_ref)\n",
    "\n",
    "    df = client1.list_rows(table).to_dataframe()\n",
    "\n",
    "    client2 = storage.Client()\n",
    "    bucket = client2.get_bucket(bucket_name)\n",
    "\n",
    "    bucket.blob(processed_file_name).upload_from_string(df.to_csv(), 'text/csv')\n",
    "    output_uri = f'gs://{bucket_name}/{processed_file_name}'\n",
    "\n",
    "    return output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "49548d15-7db8-4656-9ec5-96c845cdc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"pandas\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "\n",
    "def get_batch_prediction(\n",
    "        inference_dataset_source: str,\n",
    "        source_bucket: str,\n",
    "        inference_bucket: str,\n",
    "        project: str,\n",
    "        location: str,\n",
    "        model_parent: str) -> bool:\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import storage\n",
    "    import time\n",
    "    import calendar\n",
    "    import json\n",
    "    from datetime import date\n",
    "    import pandas as pd\n",
    "\n",
    "    ### HELPER FUNCTIONS\n",
    "    def write_string_to_gcs_txt(string ,file_name, bucket_name):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.upload_from_string(string)\n",
    "\n",
    "    def upload_blob(source_file_name, destination_blob_name, bucket_name):\n",
    "      \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "      storage_client = storage.Client()\n",
    "      bucket = storage_client.get_bucket(bucket_name)\n",
    "      blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "      blob.upload_from_filename(source_file_name)\n",
    "\n",
    "      print('File {} uploaded to {}.'.format(\n",
    "          source_file_name,\n",
    "          destination_blob_name))\n",
    "\n",
    "    def list_blobs(bucket_name , prefix = None):\n",
    "        \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix = prefix)\n",
    "\n",
    "        # Note: The call returns a response only when the iterator is consumed.\n",
    "        for blob in blobs:\n",
    "            print(blob.name)\n",
    "\n",
    "    def create_batch_prediction_job_sample(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        model_resource_name: str,\n",
    "        job_display_name: str,\n",
    "        gcs_source: str,\n",
    "        gcs_destination: str,\n",
    "        sync: bool = True,\n",
    "                            ):\n",
    "\n",
    "        import pandas as pd\n",
    "        import google\n",
    "        from google.cloud import storage\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        import time\n",
    "        import calendar\n",
    "        import json\n",
    "        from datetime import date\n",
    "\n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "        batch_prediction_job = my_model.batch_predict(\n",
    "            job_display_name=job_display_name,\n",
    "            gcs_source=gcs_source,\n",
    "            gcs_destination_prefix=gcs_destination,\n",
    "            sync=sync,\n",
    "        )\n",
    "\n",
    "        batch_prediction_job.wait()\n",
    "\n",
    "        print(batch_prediction_job.display_name)\n",
    "        print(batch_prediction_job.resource_name)\n",
    "        print(batch_prediction_job.state)\n",
    "        \n",
    "        return batch_prediction_job\n",
    "\n",
    "\n",
    "############################################################################################################################################\n",
    "\n",
    "        #Creating unique filename\n",
    "        current_GMT = time.gmtime()\n",
    "        ts = calendar.timegm(current_GMT)\n",
    "        filename = inference_dataset_source.split('/')[-1].rstrip('.csv') + str(ts)\n",
    "        todays_date = date.today().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "\n",
    "        inference_dataset = pd.read_csv(inference_dataset_source, header=None)\n",
    "        json_list = []\n",
    "        json_input_filename = 'output.jsonl'\n",
    "\n",
    "        for i in range(len(inference_dataset)):\n",
    "            write_string_to_gcs_txt(inference_dataset.iloc[i][0], f'inference-files/{todays_date}/{i}.txt',inference_bucket)\n",
    "            json_list.append({'content': f'gs://nlp-batch-prediction-test/inference-files/{todays_date}/{i}.txt', 'mimeType': 'text/plain'})\n",
    "\n",
    "        #Creates and loads JSONL from inference dataset to gcs\n",
    "        with open(json_input_filename, 'w') as outfile:\n",
    "            for entry in json_list: \n",
    "                json.dump(entry, outfile)\n",
    "                outfile.write('\\n')    \n",
    "        upload_blob(json_input_filename, f'cleaned_data/{json_input_filename}', source_bucket)\n",
    "\n",
    "        batch_input = f'gs://mlai-nlp/cleaned_data/{json_input_filename}'\n",
    "\n",
    "        # Perform batch prediction\n",
    "        create_batch_prediction_job_sample(\n",
    "            project = project,\n",
    "            location = location,\n",
    "            model_resource_name = model_parent,\n",
    "            job_display_name = 'test_predict',\n",
    "            gcs_source = batch_input,\n",
    "            gcs_destination = 'gs://mlai-nlp/cleaned_data/batch_prediction/results',\n",
    "            sync = False)\n",
    "\n",
    "        #Searches bucket of prediction results, appends blobs to blob_list\n",
    "        blobs = storage.Client().list_blobs(source_bucket, prefix = 'cleaned_data/batch_prediction/results/')\n",
    "        blob_list = []\n",
    "        for i in blobs:\n",
    "            blob_list.append(i.name)\n",
    "\n",
    "        #Finds the latest batch_prediction\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(source_bucket)\n",
    "        blob = bucket.blob(blob_list[-1])\n",
    "\n",
    "        #Reads the batch_prediction\n",
    "        with blob.open(\"r\") as file:\n",
    "            batch_prediction_output = file.read()\n",
    "\n",
    "        #Iterates through batch prediction to append the text index and predictions \n",
    "        batch_prediction_output_list = batch_prediction_output.split('\\n')\n",
    "        txt_index = []\n",
    "        prediction = []\n",
    "        for i in range(len(batch_prediction_output_list)-1):\n",
    "            txt_index.append(batch_prediction_output_list[i].split('.txt')[0][-1])\n",
    "            prediction.append(batch_prediction_output_list[i].split(\":\")[-1][0])\n",
    "\n",
    "        #Creates batch prediction dataframe\n",
    "        batch_predict_df = pd.DataFrame({'txt_index': txt_index , 'prediction' : prediction})\n",
    "\n",
    "        #Merges batch prediction dataframe to original inference dataset based off the indexing. This creates a final batch prediciton dataframe consisting of the original text and the predictionss\n",
    "        batch_predict_df = batch_predict_df.sort_values('txt_index')\n",
    "        batch_predict_df = batch_predict_df.reset_index()\n",
    "        inference_dataset = inference_dataset.rename(columns={0: \"Text\"})['Text']\n",
    "        batch_predict_final_df = pd.concat([inference_dataset,batch_predict_df['prediction']], axis=1)\n",
    "\n",
    "        #Loads DataFrames to BQ\n",
    "        load_dataframe_to_bigquery(batch_predict_final_df, table_id=f'{project}.{bq_dataset}.batch_prediction')\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f98923b8-2534-46b0-bc12-fa7f9b91702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/526415775648/locations/europe-west4/pipelineJobs/inference-20230215232113\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/526415775648/locations/europe-west4/pipelineJobs/inference-20230215232113')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/inference-20230215232113?project=526415775648\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    name=pipeline_name,\n",
    "    description='inference pipeline'\n",
    ")\n",
    "def inference_pipeline(project: str,\n",
    "                      location: str,\n",
    "                      bucket_name: str,                       \n",
    "                      model_display_name: str,\n",
    "                      service_location: str,\n",
    "                      is_inference_data: bool,\n",
    "                      gcs_file: str,\n",
    "                      bq_raw_table: str,\n",
    "                      bq_processed_table: str,\n",
    "                      bq_dataset: str,\n",
    "                      preprocess_data_job_name: str,\n",
    "                      preprocess_data_image_uri: str,\n",
    "                      preprocess_data_machine_type: str,\n",
    "                      processed_file_name: str,                      \n",
    "                      service_account: str,\n",
    "                      output_csv: str):\n",
    "\n",
    "    get_model_task = get_model_resource_name(project=project,\n",
    "                                            location=location,\n",
    "                                            model_display_name=model_display_name)\n",
    "    \n",
    "    #get_model_number_task = get_model_number(model_parent = get_model_task.output)processed_file_name\n",
    "    \n",
    "    preprocess_task = preprocess_data(project=project,\n",
    "                                location=location,\n",
    "                                is_inference_data=is_inference_data,\n",
    "                                service_location=service_location,\n",
    "                                gcs_file=gcs_file,\n",
    "                                bq_raw_table=bq_raw_table,\n",
    "                                bq_processed_table=bq_processed_table,\n",
    "                                bq_dataset=bq_dataset,\n",
    "                                preprocess_data_job_name=preprocess_data_job_name,\n",
    "                                preprocess_data_image_uri=preprocess_data_image_uri,\n",
    "                                preprocess_data_machine_type=preprocess_data_machine_type,\n",
    "                                service_account=service_account,\n",
    "                                output_csv = output_csv)\n",
    "    \n",
    "    bq_table_to_csv_task = bq_table_to_csv(project=project,\n",
    "                    bq_dataset=bq_dataset,\n",
    "                    bq_processed_table = preprocess_task.output,                                           \n",
    "                    bucket_name = bucket_name,\n",
    "                    processed_file_name = processed_file_name\n",
    "                    )\n",
    "\n",
    "    get_batch_prediction_task =  get_batch_prediction(  inference_dataset_source = bq_table_to_csv_task.output,\n",
    "                                                        source_bucket = bucket_name,\n",
    "                                                        inference_bucket = inference_bucket,\n",
    "                                                        project = project,\n",
    "                                                        location = location,\n",
    "                                                        model_parent = get_model_task.output)\n",
    "\n",
    "if RUN_PIPELINE:\n",
    "    # Compile pipeline\n",
    "    pipeline_func = inference_pipeline\n",
    "    pipeline_filename = pipeline_func.__name__ + '.json'\n",
    "    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "    #specify our pipeline parameters for our job:\n",
    "    PIPELINE_PARAMETERS = {\n",
    "                            'project': project,\n",
    "                            'location': location,\n",
    "                            'bucket_name': bucket_name,\n",
    "                            'model_display_name': model_display_name,\n",
    "                            'service_account':service_account,\n",
    "                            'service_location': service_location,\n",
    "                            'is_inference_data': is_inference_data,\n",
    "                            'gcs_file': gcs_file,\n",
    "                            'bq_raw_table': bq_raw_table,\n",
    "                            'bq_processed_table': bq_processed_table,\n",
    "                            'bq_dataset': bq_dataset,\n",
    "                            'preprocess_data_job_name': preprocess_data_job_name,\n",
    "                            'preprocess_data_image_uri': preprocess_data_image_uri,\n",
    "                            'preprocess_data_machine_type': preprocess_data_machine_type,\n",
    "                            'processed_file_name': processed_file_name,     \n",
    "                            'output_csv': output_csv\n",
    "    }\n",
    "\n",
    "    display_name = f'{pipeline_name}-{pipeline_runtime}'\n",
    "    job = aiplatform.PipelineJob(display_name=display_name,\n",
    "                                 template_path=pipeline_filename,\n",
    "                                 job_id=display_name,\n",
    "                                 pipeline_root=f'gs://{bucket_name}',\n",
    "                                 parameter_values=PIPELINE_PARAMETERS,\n",
    "                                 location = location,enable_caching=False\n",
    "    )\n",
    "\n",
    "    job.submit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37f5b4-f827-48f3-aca8-b176cd8ce5e5",
   "metadata": {},
   "source": [
    "@pipeline(\n",
    "    name=pipeline_name,\n",
    "    description='inference pipeline'\n",
    ")\n",
    "def inference_pipeline(project: str,\n",
    "                    bq_dataset: str,\n",
    "                    bq_processed_table: str):\n",
    "\n",
    "    \n",
    "    bq_table_to_df_task = bq_table_to_df(project=project,\n",
    "                    bq_dataset=bq_dataset,\n",
    "                    bq_processed_table = bq_processed_table\n",
    "                    )\n",
    "\n",
    "if RUN_PIPELINE:\n",
    "    # Compile pipeline\n",
    "    pipeline_func = inference_pipeline\n",
    "    pipeline_filename = pipeline_func.__name__ + '.json'\n",
    "    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "    #specify our pipeline parameters for our job:\n",
    "    PIPELINE_PARAMETERS = {\n",
    "                            'project': project,\n",
    "                            'bq_dataset': bq_dataset,\n",
    "                            'bq_processed_table': bq_processed_table\n",
    "    }\n",
    "\n",
    "    display_name = f'{pipeline_name}-{pipeline_runtime}'\n",
    "    job = aiplatform.PipelineJob(display_name=display_name,\n",
    "                                 template_path=pipeline_filename,\n",
    "                                 job_id=display_name,\n",
    "                                 pipeline_root=f'gs://{bucket_name}',\n",
    "                                 parameter_values=PIPELINE_PARAMETERS,\n",
    "                                 location = location,enable_caching=True\n",
    "    )\n",
    "\n",
    "    job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b7c85-7a99-43d7-ab7c-861f93561b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc95e6-001a-47c5-b842-ea2fb81291c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5e195-2bf0-4b70-a37d-d63c1a0c79a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
